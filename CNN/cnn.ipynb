{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CNN Implementation for Cat Breed Classification\n",
                "\n",
                "This notebook implements a Convolutional Neural Network (CNN) to classify cat breeds.\n",
                "\n",
                "**Objectives:**\n",
                "1.  Define a CNN with Batch Normalization and Dropout layers.\n",
                "2.  Compare the performance of different kernel sizes (e.g., 3x3, 5x5, 7x7).\n",
                "3.  Select the best kernel size and visualize Training vs. Validation loss and accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "from torchvision import datasets, transforms\n",
                "from torchvision import models\n",
                "from torch.utils.data import DataLoader\n",
                "import time, sys\n",
                "\n",
                "# Device configuration\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# helper for estimated completion time\n",
                "def format_time(seconds):\n",
                "    m, s = divmod(int(seconds), 60)\n",
                "    h, m = divmod(m, 60)\n",
                "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
                "\n",
                "def _progress_bar(current, total, start_time, prefix=''):\n",
                "    bar_len = 30\n",
                "    if total > 0:\n",
                "        filled_len = int(round(bar_len * current / float(total)))\n",
                "    else:\n",
                "        filled_len = 0\n",
                "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
                "\n",
                "    elapsed = time.time() - start_time\n",
                "    if elapsed > 0 and current > 0:\n",
                "        rate = current / elapsed\n",
                "        eta_seconds = (total - current) / rate\n",
                "    else:\n",
                "        eta_seconds = 0\n",
                "\n",
                "    eta_str = format_time(eta_seconds)\n",
                "\n",
                "    sys.stdout.write(f'\\r{prefix} [{bar}] {current}/{total} | ETA: {eta_str}')\n",
                "    sys.stdout.flush()\n",
                "    if current == total:\n",
                "        sys.stdout.write('\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Classes: 66\n",
                        "Training samples: 18054\n",
                        "Testing samples: 2257\n"
                    ]
                }
            ],
            "source": [
                "# Hyperparameters\n",
                "BATCH_SIZE = 64\n",
                "LEARNING_RATE = 0.001\n",
                "NUM_EPOCHS = 10  \n",
                "TARGET_SIZE = (224, 224)\n",
                "\n",
                "# Data Directories\n",
                "TRAIN_DIR = 'data/train'\n",
                "TEST_DIR = 'data/test'\n",
                "\n",
                "# Transforms - Updated for better accuracy\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize(256),\n",
                "    transforms.CenterCrop(224),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(\n",
                "        mean=[0.485, 0.456, 0.406],  \n",
                "        std=[0.229, 0.224, 0.225]\n",
                "    )\n",
                "])\n",
                "\n",
                "# Load Datasets\n",
                "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=transform)\n",
                "test_dataset = datasets.ImageFolder(root=TEST_DIR, transform=transform)\n",
                "\n",
                "# Data Loaders\n",
                "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f'Classes: {len(train_dataset.classes)}')\n",
                "print(f'Training samples: {len(train_dataset)}')\n",
                "print(f'Testing samples: {len(test_dataset)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Definition\n",
                "Includes batch normalization and dropout layers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CatBreedCNN(nn.Module):\n",
                "    def __init__(self, num_classes=66, kernel_size=3):\n",
                "        super(CatBreedCNN, self).__init__()\n",
                "        \n",
                "        padding = kernel_size // 2\n",
                "        \n",
                "        # Block 1\n",
                "        self.conv1 = nn.Conv2d(3, 64, kernel_size, padding=padding)\n",
                "        self.bn1 = nn.BatchNorm2d(64)\n",
                "        self.conv2 = nn.Conv2d(64, 64, kernel_size, padding=padding)\n",
                "        self.bn2 = nn.BatchNorm2d(64)\n",
                "        \n",
                "        # Block 2\n",
                "        self.conv3 = nn.Conv2d(64, 128, kernel_size, padding=padding)\n",
                "        self.bn3 = nn.BatchNorm2d(128)\n",
                "        self.conv4 = nn.Conv2d(128, 128, kernel_size, padding=padding)\n",
                "        self.bn4 = nn.BatchNorm2d(128)\n",
                "        \n",
                "        # Block 3\n",
                "        self.conv5 = nn.Conv2d(128, 256, kernel_size, padding=padding)\n",
                "        self.bn5 = nn.BatchNorm2d(256)\n",
                "        self.conv6 = nn.Conv2d(256, 256, kernel_size, padding=padding)\n",
                "        self.bn6 = nn.BatchNorm2d(256)\n",
                "        \n",
                "        # Block 4\n",
                "        self.conv7 = nn.Conv2d(256, 512, kernel_size, padding=padding)\n",
                "        self.bn7 = nn.BatchNorm2d(512)\n",
                "        self.conv8 = nn.Conv2d(512, 512, kernel_size, padding=padding)\n",
                "        self.bn8 = nn.BatchNorm2d(512)\n",
                "        \n",
                "        self.pool = nn.MaxPool2d(2, 2)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "        self.relu = nn.ReLU()\n",
                "        \n",
                "        # Adaptive pooling to handle any input size\n",
                "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
                "        \n",
                "        # Fully connected layers\n",
                "        self.fc1 = nn.Linear(512 * 7 * 7, 1024)\n",
                "        self.fc2 = nn.Linear(1024, 512)\n",
                "        self.fc3 = nn.Linear(512, num_classes)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # Block 1\n",
                "        x = self.relu(self.bn1(self.conv1(x)))\n",
                "        x = self.relu(self.bn2(self.conv2(x)))\n",
                "        x = self.pool(x)\n",
                "        \n",
                "        # Block 2\n",
                "        x = self.relu(self.bn3(self.conv3(x)))\n",
                "        x = self.relu(self.bn4(self.conv4(x)))\n",
                "        x = self.pool(x)\n",
                "        \n",
                "        # Block 3\n",
                "        x = self.relu(self.bn5(self.conv5(x)))\n",
                "        x = self.relu(self.bn6(self.conv6(x)))\n",
                "        x = self.pool(x)\n",
                "        \n",
                "        # Block 4\n",
                "        x = self.relu(self.bn7(self.conv7(x)))\n",
                "        x = self.relu(self.bn8(self.conv8(x)))\n",
                "        x = self.pool(x)\n",
                "        \n",
                "        # Adaptive pooling\n",
                "        x = self.adaptive_pool(x)\n",
                "        \n",
                "        # Flatten\n",
                "        x = x.view(x.size(0), -1)\n",
                "        \n",
                "        # FC layers\n",
                "        x = self.dropout(x)\n",
                "        x = self.relu(self.fc1(x))\n",
                "        x = self.dropout(x)\n",
                "        x = self.relu(self.fc2(x))\n",
                "        x = self.fc3(x)\n",
                "        \n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Function\n",
                "Validation tracking during model training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS):\n",
                "    model.to(device)\n",
                "    history = {\n",
                "        'train_loss': [], 'train_acc': [],\n",
                "        'val_loss': [], 'val_acc': []\n",
                "    }\n",
                "    \n",
                "    print(f\"Training on {device}...\")\n",
                "    \n",
                "    for epoch in range(num_epochs):\n",
                "        \n",
                "        # --- Training Phase ---\n",
                "        model.train()\n",
                "        train_loss = 0.0\n",
                "        train_correct = 0\n",
                "        train_total = 0\n",
                "        \n",
                "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}] Training: \", end='', flush=True)\n",
                "        train_phase_start = time.time()\n",
                "        for batch_idx, (images, labels) in enumerate(train_loader, 1):\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            train_loss += loss.item()\n",
                "            _, predicted = torch.max(outputs.data, 1)\n",
                "            train_total += labels.size(0)\n",
                "            train_correct += (predicted == labels).sum().item()\n",
                "            \n",
                "            _progress_bar(batch_idx, len(train_loader), train_phase_start, prefix='Training')\n",
                "            \n",
                "        if len(train_loader) > 0:\n",
                "            avg_train_loss = train_loss / len(train_loader)\n",
                "        else:\n",
                "            avg_train_loss = 0.0\n",
                "            \n",
                "        if train_total > 0:\n",
                "            avg_train_acc = 100 * train_correct / train_total\n",
                "        else:\n",
                "            avg_train_acc = 0.0\n",
                "        \n",
                "        # --- Validation Phase ---\n",
                "        model.eval()\n",
                "        val_loss = 0.0\n",
                "        val_correct = 0\n",
                "        val_total = 0\n",
                "        \n",
                "        print(f\"\\nValidating: \", end='', flush=True)\n",
                "        val_phase_start = time.time()\n",
                "        with torch.no_grad():\n",
                "            for batch_idx, (images, labels) in enumerate(val_loader, 1):\n",
                "                images, labels = images.to(device), labels.to(device)\n",
                "                outputs = model(images)\n",
                "                loss = criterion(outputs, labels)\n",
                "                \n",
                "                val_loss += loss.item()\n",
                "                _, predicted = torch.max(outputs.data, 1)\n",
                "                val_total += labels.size(0)\n",
                "                val_correct += (predicted == labels).sum().item()\n",
                "                \n",
                "                _progress_bar(batch_idx, len(val_loader), val_phase_start, prefix='Validation')\n",
                "                \n",
                "        if len(val_loader) > 0:\n",
                "            avg_val_loss = val_loss / len(val_loader)\n",
                "        else:\n",
                "            avg_val_loss = 0.0\n",
                "            \n",
                "        if val_total > 0:\n",
                "            avg_val_acc = 100 * val_correct / val_total\n",
                "        else:\n",
                "            avg_val_acc = 0.0\n",
                "        \n",
                "        # --- record history ---\n",
                "        history['train_loss'].append(avg_train_loss)\n",
                "        history['train_acc'].append(avg_train_acc)\n",
                "        history['val_loss'].append(avg_val_loss)\n",
                "        history['val_acc'].append(avg_val_acc)\n",
                "        \n",
                "        print(\n",
                "        f\"\\n  ── Epoch {epoch+1}/{num_epochs} Summary ─────────────────────────────\"\n",
                "        f\"\\n  Train | Loss: {avg_train_loss:.4f} | Acc: {avg_train_acc:6.2f}%\"\n",
                "        f\"\\n  Valid | Loss: {avg_val_loss:.4f} | Acc: {avg_val_acc:6.2f}%\"\n",
                "        f\"\\n  ──────────────────────────────────────────────────────────────────────\"\n",
                "        )\n",
                "              \n",
                "    return history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Experiments: Kernel Sizes\n",
                "Comparing Kernel Sizes: 3, 5, and 7."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "EXPERIMENT 1/3: Kernel Size = 3\n",
                        "============================================================\n",
                        "Training on cpu...\n",
                        "\n",
                        "Training [================--------------] 149/283 | ETA: 01:16:56"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     15\u001b[39m                                                factor=\u001b[32m0.5\u001b[39m, patience=\u001b[32m2\u001b[39m)\n\u001b[32m     16\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m history = train_and_validate(model, train_loader, test_loader, criterion, optimizer, num_epochs=NUM_EPOCHS)\n\u001b[32m     19\u001b[39m results[k] = history\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain_and_validate\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[39m\n\u001b[32m     24\u001b[39m outputs = model(images)\n\u001b[32m     25\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m loss.backward()\n\u001b[32m     27\u001b[39m optimizer.step()\n\u001b[32m     29\u001b[39m train_loss += loss.item()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hurri\\anaconda3\\envs\\cs171\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m torch.autograd.backward(\n\u001b[32m    648\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    649\u001b[39m )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hurri\\anaconda3\\envs\\cs171\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m _engine_run_backward(\n\u001b[32m    355\u001b[39m     tensors,\n\u001b[32m    356\u001b[39m     grad_tensors_,\n\u001b[32m    357\u001b[39m     retain_graph,\n\u001b[32m    358\u001b[39m     create_graph,\n\u001b[32m    359\u001b[39m     inputs_tuple,\n\u001b[32m    360\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    361\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    362\u001b[39m )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hurri\\anaconda3\\envs\\cs171\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    830\u001b[39m         t_outputs, *args, **kwargs\n\u001b[32m    831\u001b[39m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "kernel_sizes = [3, 5, 7]\n",
                "results = {}\n",
                "\n",
                "total_experiments = len(kernel_sizes)\n",
                "\n",
                "for exp_idx, k in enumerate(kernel_sizes, 1):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"EXPERIMENT {exp_idx}/{total_experiments}: Kernel Size = {k}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    model = CatBreedCNN(kernel_size=k)\n",
                "    # Lower learning rate for fine-tuning\n",
                "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
                "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', \n",
                "                                                   factor=0.5, patience=2)\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    \n",
                "    history = train_and_validate(model, train_loader, test_loader, criterion, optimizer, num_epochs=NUM_EPOCHS)\n",
                "    results[k] = history\n",
                "    \n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Experiment {exp_idx} Complete!\")\n",
                "    print(f\"{'='*60}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Results & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# comparing kernel sizes based on validation accuracy\n",
                "plt.figure(figsize=(10, 6))\n",
                "for k in kernel_sizes:\n",
                "    plt.plot(results[k]['val_acc'], label=f'Kernel Size {k}')\n",
                "plt.title('Validation Accuracy vs. Epochs for Different Kernel Sizes')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Validation Accuracy (%)')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "# find best kernel size\n",
                "best_k = max(results, key=lambda k: max(results[k]['val_acc']))\n",
                "print(f\"\\nBest Kernel Size based on max validation accuracy: {best_k}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# plot best model loss and accuracy\n",
                "best_history = results[best_k]\n",
                "\n",
                "plt.figure(figsize=(14, 5))\n",
                "\n",
                "# loss\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(best_history['train_loss'], label='Train Loss')\n",
                "plt.plot(best_history['val_loss'], label='Validation Loss')\n",
                "plt.title(f'Best Model (Kernel {best_k}): Loss')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "\n",
                "# accuracy\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(best_history['train_acc'], label='Train Accuracy')\n",
                "plt.plot(best_history['val_acc'], label='Validation Accuracy')\n",
                "plt.title(f'Best Model (Kernel {best_k}): Accuracy')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy (%)')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "cs171",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
