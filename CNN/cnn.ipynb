{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CNN Implementation for Cat Breed Classification\n",
                "\n",
                "This notebook implements a Convolutional Neural Network (CNN) to classify cat breeds.\n",
                "\n",
                "**Objectives:**\n",
                "1.  **Model Architecture**: Define a CNN with Batch Normalization and Dropout layers.\n",
                "2.  **Kernel Size Experiment**: Compare the performance of different kernel sizes (e.g., 3x3, 5x5, 7x7).\n",
                "3.  **Evaluation**: Select the best kernel size and visualize Training vs. Validation loss and accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "from torchvision import datasets, transforms\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "# Device configuration\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Classes: 66\n",
                        "Training samples: 18054\n",
                        "Testing samples: 2257\n"
                    ]
                }
            ],
            "source": [
                "# Hyperparameters\n",
                "BATCH_SIZE = 32\n",
                "LEARNING_RATE = 0.001\n",
                "NUM_EPOCHS = 10  # Increased slightly to see convergence differences\n",
                "TARGET_SIZE = (224, 224)\n",
                "\n",
                "# Data Directories\n",
                "TRAIN_DIR = 'data/train'\n",
                "TEST_DIR = 'data/test'\n",
                "\n",
                "# Transforms\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "])\n",
                "\n",
                "# Load Datasets\n",
                "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=transform)\n",
                "test_dataset = datasets.ImageFolder(root=TEST_DIR, transform=transform)\n",
                "\n",
                "# Data Loaders\n",
                "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f'Classes: {len(train_dataset.classes)}')\n",
                "print(f'Training samples: {len(train_dataset)}')\n",
                "print(f'Testing samples: {len(test_dataset)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Definition\n",
                "Defining a CNN class that includes **Batch Normalization** and **Dropout** by default. The kernel size is parameterized."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CatBreedCNN(nn.Module):\n",
                "    def __init__(self, num_classes=66, kernel_size=3):\n",
                "        super(CatBreedCNN, self).__init__()\n",
                "        # Calculate padding to maintain spatial dimensions (same padding)\n",
                "        # padding = (kernel_size - 1) / 2\n",
                "        padding = kernel_size // 2\n",
                "        \n",
                "        # Layer 1\n",
                "        self.conv1 = nn.Conv2d(3, 32, kernel_size=kernel_size, padding=padding)\n",
                "        self.bn1 = nn.BatchNorm2d(32)\n",
                "        \n",
                "        # Layer 2\n",
                "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=padding)\n",
                "        self.bn2 = nn.BatchNorm2d(64)\n",
                "        \n",
                "        # Layer 3\n",
                "        self.conv3 = nn.Conv2d(64, 128, kernel_size=kernel_size, padding=padding)\n",
                "        self.bn3 = nn.BatchNorm2d(128)\n",
                "        \n",
                "        self.pool = nn.MaxPool2d(2, 2)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "        self.relu = nn.ReLU()\n",
                "        \n",
                "        # Calculate input size for FC layer\n",
                "        # Input: 224x224\n",
                "        # After Pool 1: 112x112\n",
                "        # After Pool 2: 56x56\n",
                "        # After Pool 3: 28x28\n",
                "        self.fc_input_size = 128 * 28 * 28\n",
                "        \n",
                "        self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
                "        self.fc2 = nn.Linear(512, num_classes)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # Block 1\n",
                "        x = self.conv1(x)\n",
                "        x = self.bn1(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.pool(x)\n",
                "        \n",
                "        # Block 2\n",
                "        x = self.conv2(x)\n",
                "        x = self.bn2(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.pool(x)\n",
                "        \n",
                "        # Block 3\n",
                "        x = self.conv3(x)\n",
                "        x = self.bn3(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.pool(x)\n",
                "        \n",
                "        # Classifier\n",
                "        x = x.view(-1, self.fc_input_size)\n",
                "        x = self.dropout(x)\n",
                "        x = self.fc1(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.fc2(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Function\n",
                "Includes validation tracking at each epoch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS):\n",
                "    model.to(device)\n",
                "    history = {\n",
                "        'train_loss': [], 'train_acc': [],\n",
                "        'val_loss': [], 'val_acc': []\n",
                "    }\n",
                "    \n",
                "    print(f\"Training on {device}...\")\n",
                "    \n",
                "    for epoch in range(num_epochs):\n",
                "        # --- Training Phase ---\n",
                "        model.train()\n",
                "        train_loss = 0.0\n",
                "        train_correct = 0\n",
                "        train_total = 0\n",
                "        \n",
                "        for images, labels in train_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            train_loss += loss.item()\n",
                "            _, predicted = torch.max(outputs.data, 1)\n",
                "            train_total += labels.size(0)\n",
                "            train_correct += (predicted == labels).sum().item()\n",
                "            \n",
                "        avg_train_loss = train_loss / len(train_loader)\n",
                "        avg_train_acc = 100 * train_correct / train_total\n",
                "        \n",
                "        # --- Validation Phase ---\n",
                "        model.eval()\n",
                "        val_loss = 0.0\n",
                "        val_correct = 0\n",
                "        val_total = 0\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for images, labels in val_loader:\n",
                "                images, labels = images.to(device), labels.to(device)\n",
                "                outputs = model(images)\n",
                "                loss = criterion(outputs, labels)\n",
                "                \n",
                "                val_loss += loss.item()\n",
                "                _, predicted = torch.max(outputs.data, 1)\n",
                "                val_total += labels.size(0)\n",
                "                val_correct += (predicted == labels).sum().item()\n",
                "                \n",
                "        avg_val_loss = val_loss / len(val_loader)\n",
                "        avg_val_acc = 100 * val_correct / val_total\n",
                "        \n",
                "        # --- Record History ---\n",
                "        history['train_loss'].append(avg_train_loss)\n",
                "        history['train_acc'].append(avg_train_acc)\n",
                "        history['val_loss'].append(avg_val_loss)\n",
                "        history['val_acc'].append(avg_val_acc)\n",
                "        \n",
                "        print(f'Epoch [{epoch+1}/{num_epochs}]: '\n",
                "              f'Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.2f}% | '\n",
                "              f'Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.2f}%')\n",
                "              \n",
                "    return history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Experiments: Kernel Sizes\n",
                "Comparing Kernel Sizes: 3, 5, and 7."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "========================================\n",
                        "Experiment: Kernel Size = 3\n",
                        "========================================\n",
                        "Training on cpu...\n"
                    ]
                }
            ],
            "source": [
                "kernel_sizes = [3, 5, 7]\n",
                "results = {}\n",
                "\n",
                "for k in kernel_sizes:\n",
                "    print(f\"\\n{'='*40}\")\n",
                "    print(f\"Experiment: Kernel Size = {k}\")\n",
                "    print(f\"{'='*40}\")\n",
                "    \n",
                "    model = CatBreedCNN(kernel_size=k)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    \n",
                "    history = train_and_validate(model, train_loader, test_loader, criterion, optimizer, num_epochs=NUM_EPOCHS)\n",
                "    results[k] = history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Results & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Compare Kernel Sizes (Validation Accuracy)\n",
                "plt.figure(figsize=(10, 6))\n",
                "for k in kernel_sizes:\n",
                "    plt.plot(results[k]['val_acc'], label=f'Kernel Size {k}')\n",
                "plt.title('Validation Accuracy vs. Epochs for Different Kernel Sizes')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Validation Accuracy (%)')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "# Determine Best Kernel Size\n",
                "best_k = max(results, key=lambda k: max(results[k]['val_acc']))\n",
                "print(f\"\\nBest Kernel Size based on max validation accuracy: {best_k}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Detailed Plots for Best Model\n",
                "best_history = results[best_k]\n",
                "\n",
                "plt.figure(figsize=(14, 5))\n",
                "\n",
                "# Loss Plot\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(best_history['train_loss'], label='Train Loss')\n",
                "plt.plot(best_history['val_loss'], label='Validation Loss')\n",
                "plt.title(f'Best Model (Kernel {best_k}): Loss')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "\n",
                "# Accuracy Plot\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(best_history['train_acc'], label='Train Accuracy')\n",
                "plt.plot(best_history['val_acc'], label='Validation Accuracy')\n",
                "plt.title(f'Best Model (Kernel {best_k}): Accuracy')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy (%)')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "cs171",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
